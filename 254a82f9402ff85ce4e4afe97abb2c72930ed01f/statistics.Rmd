---
title: "Statistics"
---

```{r eval = TRUE,  message=F, include=F, warning=F, purl=F, results="hide"}
knitr::purl('statistics.Rmd', documentation = F)
```

```{r echo=FALSE, purl=F}
xfun::embed_file('statistics.Rmd')
```

```{r echo=FALSE, purl=F}
xfun::embed_file('statistics.R')
```

![](Figures/Sadistics.png)

```{r,  eval=T, warning=F, message=F}
library (psych)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(car)
```

# Descriptive

## Basic description

We will go back to our dummy `students` data set. Do know about the pivot table functionality in excel? We already mentioned that the same could be easily accomplished in R such as here using the functions  as `describe` or `describeBy`. 

```{r,  eval=T}
students<-read.table('https://www.dipintothereef.com/uploads/3/7/3/5/37359245/students.txt',header=T, sep="\t", dec='.') # inspect the object created

write.table(students, file = "Data/students.txt", sep = "\t", row.names=T)
```

+ Make a quick `summary` of our data set in R.

```{r,  eval=T}
summary(students)
```

+ A more detailed summary can be obtained using the function `describe` from the package `psych`.

```{r,  eval=T}
describe(students)
```

+ The function `describeBy` further provides a group-wise summary of the dataset.

```{r,  eval=T}
describeBy (students,students$gender)
describeBy (students, list(students$gender,students$population))
```

## Counts and proportions

Simple tabulation (count) summary are made using`table`

```{r,  eval=T}
# One variables
table(students$gender)
prop.table (table(students$gender))

# two variables
table(students$gender, students$shoesize)
prop.table (table(students$gender, students$shoesize))

# three variables, with nicer formatting
ftable(students$gender, students$shoesize,students$population)
```

## `mean`, `median`, `sd`, etc.

You already know how to apply basic function on a vector, create a filter and use it on a subgroup:

```{r,  eval=T}
mean(students$height)
ind.male <- students$gender == 'male'
mean(students$height[ind.male])
```

Common descriptive statistics can be combined in the `aggregate` and `apply` functions. The 'big' difference between the two is  that  the second argument of `aggregate` must be a list while `tapply` can (so not mandatory) be a list and that the output of `aggregate` is a data frame while the one of `tapply` is an array.   

```{r,  eval=T}
aggregate(students$height,list (students$gender),median)
tapply(students$height,students$gender, median)
```

The wide family of `apply` functions  represents common alternatives to loops. You can read more about these functions online.




<p class="comment">
**Practice S1:** Using the data 'iris' explore how the four traits vary among speces. (1) Use boxplots and make a summary of the data set by species. (2) Calculate the number of observations for each trait and species (many possibilities, be creative). You should get a table of 3 species x 4 traits filled with the values "50". (3) Calculate median of each variable by `Species`, then calculate the mean by `Species` for the `Sepal.Length` only
</p>

```{r, code_folding = 'Show Solution',  eval=F}
plot1 <-ggplot(iris, aes(x=Species, y=Sepal.Length)) + 
  geom_boxplot()
  
plot2 <-ggplot(iris, aes(x=Species, y=Sepal.Width)) + 
  geom_boxplot() 
  
plot3 <-ggplot(iris, aes(x=Species, y=Petal.Length)) + 
  geom_boxplot() 
  
plot4 <-ggplot(iris, aes(x=Species, y=Petal.Width)) + 
  geom_boxplot() 
  
grid.arrange(plot1, plot2,plot3, plot4, ncol=2)
describeBy (iris, iris$Species)

iris %>% group_by(Species) %>% summarise_each(list(length))
aggregate(iris[,1:4],by=list(iris$Species), median)
tapply(iris$Sepal.Length , iris$Species, mean)
```

# Hypothesis testing

Two broad categories: parametric (assuming normal distribution & homoscedasticity) and non parametric (no assumption of normality).The most important is your **hypotheses**: *H0* represents the null hypothesis - an absence of difference (does not mean there is no difference), *H1* represents the alternative hypothesis - the presence of a difference.

In those tests it is important to understand the concept of degrees of freedoom: it refers to **the maximum number of logically independent values**, which are values that have the freedom to vary, in the data sample. Have a look [here](https://www.youtube.com/watch?v=N20rl2llHno&ab_channel=zedstatistics) if this notion is not clear to you

## Correlations

Let's first examine what is a correlation. It may look scary but it is actually very easy to compute (see example).

![](Figures/corr.png)

```{r class.source = "fold-show",  eval=T}
# dataset hypotheses?
x<-students$height
y<-students$shoesize
s<-students[,1:2] # a matrix
# Pearson correlation
# cor(x,y)
# cor(s)
cor.test(x,y)
```

This relationship is usually represented with it confident interval on a scatter plot.

```{r,  eval=T}
ggplot(students, aes(x = height, y = shoesize)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
```

This is a *Pearson* correlation (parametric). Non-parametric alternatives also exists such as the *Spearman*. It is based on rank, and monotonic relationship.

```{r,  eval=T}
# Spearman correlation (monotonic)
# cor(x,y, method ='spearman')
cor.test(x,y, method ='spearman')
```

The example below illustrates on the importance of running both types of correlation. 

```{r,  eval=F}
w<-(1:100)
z<-exp(x)
cor.test(w,z,method='pearson') # super low
cor.test(w,z,method='spearman') #super high
```

<p class="comment">
**Practice S2:** Change one value from the student data set and check how the t and p values are affected. Check the change in the confidence interval.
</p>

## Chi-square test

It is a goodness-of-fit test. Easier to understand with an example:

![](Figures/die.png)

```{r,  eval=T}
#Cast 240 times a die. We counted occurence of 1,2,3,4,5,6
die<-data.frame(obs=c(55,44,35,45,31,30), row.names=c('Cast1','Cast2','Cast3','Cast4','Cast5','Cast6'))
die #Is this die fair? Define H0 and H1.  
```

Assumption: all results are equally probably (H0, uniform distribution).

```{r,  eval=T}
chisq.test(die)
# I am cheating
```

A common biological application of a Chi-square test is the Hardy-Weinberg equilibrium. Genotypes at the Hardy-Weinberg equilibrium will follow: *p*2 + 2*pq* + *q*2 = 1 at panmixia (random mating, *p* and *q* represent allele frequency). In this case, we want to compare our observation to a theoretical distribution.

Let's assume 2 alleles (*A* and *T*) with observed genotypes in a population: 750 *AA*, *50* AT, 200 *TT* => *f(A)*: 0.775 / *f(T)*= 0.225. Theoretical distribution should follow: 0.60(*p2*), 0.5 (*2pq*), 0.05 (*q2*) with *p2* + *2pq* + *q2* = 1. Is our population at the HW-equilibrium (H0)?

```{r,  eval=T}
obs <- c(750, 50, 200)
exp <- c(0.60, 0.35, 0.05)
chisq.test (x=obs, p=exp)
```

Chi-square test can simply be used to compare the distribution of frequencies in two populations: 

```{r,  eval=T}
F <- matrix(nrow=4,ncol=2,data=c(33,14, 8,18,31,25,14,12))
chisq.test(F) # alternative see `fisher.test`
```

## Student t-test

Several version: one sample compared to a known mean, two samples (with variance equal - Student's or with unequal variance -Welsh's), paired (compare two dependent (e.g. before-after) samples. 

```{r,  eval=T}
# One sample
t.test (students$height, mu=170)
# Two sample (with equal variances)
t.test (students$height~students$gender, var.equal = TRUE)
# Two sample (with unequal variances, default option when using t.test) 
t.test (students$height~students$gender)
# Two sample paired t.test
t.test (students$height~students$gender, paired=T)
```

The *t* statistic is obtained as following in the case of a two samples assuming equal variances:

![](Figures/ttest.png)


<p class="comment">
**Practice S3:** Using `iris`. Does `Sepal.Lenght` differ between `setosa` and `versicolor`? Does `Sepal.Lenght` differe between `virginica`  and `versicolor`? Make a plot, define your hypotheses, test them.
</p>



```{r, code_folding = 'Show Solution',  eval=F}
set <- iris[iris$Species == "setosa", ]$Sepal.Length
ver <- iris[iris$Species == "versicolor", ]$Sepal.Length
vir <- iris[iris$Species == "virginica", ]$Sepal.Length

setver <- t.test(set, ver, paired = FALSE, alternative = "two.sided", var.equal = FALSE)
vervir <- t.test(ver, vir, paired = FALSE, alternative = "two.sided", var.equal = FALSE)
```

## Mann-Whitney and Wilcoxon tests

A non-parametric solution for the comparison of two samples: Mann-Whitney U-test (independent) or Wilcoxon W-test (dependant)

```{r class.source = "fold-show",  eval=T}
# Normality plot & test
students$height[6]<-132
students$height[10]<-310
students$height[8]<-132
students$height[9]<-210
boxplot(height~gender, students)
qqnorm(students$height) 
qqline(students$height) 
shapiro.test(students$height) # data are not normal 
wilcox.test (students$height~students$gender) 
```

## Variance tests

Variance tests are used to determine if the variances of two (or more) populations are equal. F-test are generally very sensitive to the non-normality of the data as you can imagine from the previous `boxplot`. The **homoscedasticity** (or homogeneity of variance) is an important condition in parametric statistics.

```{r, eval=T}
# Test of variance: we test HO: homogenous, H1:heterogenous
fligner.test (students$height ~ students$gender)
```

Another example using the `ToothGrowth` dataset (`car` package) on multiple groups

```{r, eval=T}
tg<-ToothGrowth
tg$dose<-factor(tg$dose)
boxplot(len~dose*supp, data=tg)
# also work with: boxplot(len ~ interaction (dose,supp), data=tg)
# or: plot(len ~ interaction (dose,supp), data=tg)
bartlett.test(len~interaction (supp,dose),data=ToothGrowth) # sensitivity non-normality +++
leveneTest(len~interaction (supp,dose),data=ToothGrowth) # sensitivity non-normality ++
fligner.test(len~interaction (supp,dose),data=ToothGrowth) # sensitivity non-normality +
```


<p class="alert">
**Practice S3:** Create your __own__ `my.t.test` function, that you will use to test the effect  of `treatment` on _length_. Applied it a for various _days_. Interpret.
</p>


